---
title: "Target Markdown and stantargets for Bayesian model validation pipelines"
author: Will Landau
output: 
  xaringan::moon_reader:
    nature:
      highlightLines: true
---

<style>
.inverse {
background-color: transparent;
text-shadow: 0 0 0px transparent;
}
.title-slide {
vertical-align: bottom !important; 
text-align: center !important;
}
.title-slide h1 {
position: absolute;
top: 0;
left: 0;
right: 0;
width: 100%;
line-height: 4em;
color: #666666;
font-size: 2em;
}
.title-slide h3 {
line-height: 2em;
color: #666666;
}
.title-slide {
background-color: white;
background-image: url('images/logos.png');
background-repeat: no-repeat;
background-size: 50%;
}
.remark-slide-content:after {
content: "Copyright Eli Lilly and Company";
position: absolute;
bottom: -5px;
left: 10px;
height: 40px;
width: 100%;
font-family: Helvetica, Arial, sans-serif;
font-size: 0.7em;
color: gray;
background-repeat: no-repeat;
background-size: contain;
}
.remark-slide-content .nocopyright:after {
content: "";
}
.small {
  font-size: 65%;
}
</style>

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  comment = "#>",
  fig.align = "center",
  fig.width = 10,
  fig.height = 7,
  out.width = "80%",
  out.height = "80%"
)
```

```{r, include = FALSE, message = FALSE}
options(
  warnPartialMatchArgs = FALSE,
  drake_clean_menu = FALSE,
  drake_make_menu = FALSE,
  htmltools.dir.version = FALSE
)
packages <- c(
  "targets",
  "keras",
  "recipes",
  "rsample",
  "tidyverse",
  "yardstick"
)
purrr::walk(
  packages,
  function(pkg) {
    suppressMessages(suppressWarnings(library(pkg, character.only = TRUE)))
  }
)
Sys.setenv(TAR_SCRIPT_ASK = "false")
tar_destroy()
```

## Demanding computation in R

* **Bayesian data analysis: JAGS, Stan, NIMBLE, `greta`**
* Deep learning: `keras`, `tensorflow`, `torch`
* Machine learning: `tidymodels`
* PK/PD: `nlmixr`, `mrgsolve`
* Clinical trial simulation: `rpact`, `Mediana`
* Statistical genomics
* Social network analysis
* Permutation tests
* Database queries: `DBI`
* ETL on large data

???

Every workflow system has tradeoffs. R Markdown is easy to use, but it struggles to handle a lot of code and a lot of runtime. On the other hand the {targets} package handles a ton of work, but it's a bit pedantic for routine data analysis.

This talk is the debut of a new system called Target Markdown, which has all the convenience of R Markdown and all the power of {targets}. I will also discuss {stantargets}, an extension to {targets} for Bayesian Statistics. 

Together, these tools make it easy to tackle daunting problems that come up a lot in the life sciences. Machine learning, MCMC, simulation, prediction, genomics, PK/PD, and database queries are just some examples.

---

## Repetition: the overlooked bane of long computation

<br>

![](./images/reality.png)

???

And even if you've done it before, it's easy to underestimate how hard it is to be on the implementation side. It's not as if you just code up all your models, let them run for a while, and then you're done. There will always be bugs to fix, follow-up questions to answer, and all sorts of reasons to rerun all of that slow code. Typically in development, this happens so frequently that you're stuck in a Sisyphean loop where you spend most of your time waiting for jobs to finish and can't seem to get hold of a complete set of current and up-to-date results. Even if the delay is only a minute, you'll feel it.

---

## Workflows have interconnected steps.

![](./images/workflow.png)

???

But what if we broke it down? A data analysis has a flow, and you can put this flow in a pipeline that begins with the data and ends with plots and summaries.

---

## If you change code or data...

![](./images/change.png)

???

If you change the code for a model, for example,

---

## ...the downstream steps are no longer valid.

![](./images/downstream.png)


???

that change invalidates everything that uses that model. The post-processing and summaries for that model need to rerun to get the latest answers. But you shouldn't have to waste time rerunning the data preprocessing and preparation steps that happen before that model, and there's no reason to waste time rerunning any of the other time-consuming models that are still up to date.

---

## Dilemma: short runtimes or reproducible results?

![](./images/decisions.png)

???

On the other hand, skipping steps that are already up to date is a difficult job for a human. It's really hard to know what you can skip and what you can't. And if you incorrectly skip a step, your results aren't reproducible anymore.

---

## Let a pipeline tool figure out what to rerun.

![](./images/pipeline_graph.png)

* Save time while ensuring computational reproducibility.
* Automatic parallel/distributed computing based on the directed acyclic graph.

???

Instead, we need a pipeline tool. If we know the inputs and outputs of each step of the pipeline, then a pipeline tool orchestrates the tasks in a directed acyclic graph and automatically makes objective decisions about what to run and what to skip. So you can adapt to bugfixes quickly without having to think about it, and you have tangible evidence of the status of each step. If the pipeline tool tells you everything is up to date, that's telling you someone else could rerun the same code and they would get the same output you have now. Arguably, that is a definition of reproducibility.

---

## Pipeline tools

<center>
<img src="./images/infographic.png" height = "125px">
</center>

* Existing pipeline tools: https://github.com/pditommaso/awesome-pipeline
* Most are language-agnostic or designed for Python or the shell.

## {targets}

* Fundamentally designed for R.
* Supports a clean, modular, function-oriented programming style.
* Abstracts files as R objects and automatically manages data.
* Surpasses the permanent limitations of its predecessor, [`drake`](https://github.com/ropensci/drake): <https://books.ropensci.org/targets/drake.html>.
* Continuation of the ideas from `remake` by Rich FitzJohn: <https://github.com/richfitz/remake>.

???

There are a lot of pipeline tools for other languages, but historically not a whole lot for R. The {targets} package, which builds on the history of {drake} and {remake}, is designed to work seamlessly within R itself, and it abstracts files as R objects so you can program more idiomatically.

---

## Challenge

* Most pipelines have a lot of user-side code.
* `targets` prefers code to be in pure user-defined functions.
* Leads to a lot of user-side software engineering.

## Solutions for Bayesian workflows

* `stantargets` reduces the volume of user-side code and automates entire validation pipelines: <https://docs.ropensci.org/stantargets/articles/simulation.html>
* Target Markdown is a comfortable interface for interactive prototyping and non-interactive pipeline construction: <https://books.ropensci.org/targets/markdown.html>

???

But there are drawbacks. Constructing a pipeline requires you to have a clear idea the exact inputs and the exact outputs of each step. You don't have to explicitly declare them like in other tools, because {targets} implicitly detects them using static code analysis, but you do have to be disciplined about the way you structure your R code. And that means writing pure functions to produce datasets, run models on those datasets, and summarize those models. That's a lot of software engineering to ask of a statistician or data analyst.

So what can we do? Well, that's actually the point of this talk. I want to share with you two major breakthroughs that democratize pipelines. 

---

## Extending {targets}

![](./images/targetopia.png)

???

The first is the R Targetopia, an emerging collection of packages like `stantargets` that produce ready-made pipelines for specialized situations. These packages already have functions and targets already built in, so as the user do not have to write nearly as much code.

---

## Target factories

* A target factory is a reusable function that creates target objects.

```{r, eval = FALSE}
target_factory <- function(file) {
  list(
    tar_target_raw("file", file, format = "file", deployment = "main"),
    tar_target_raw("data", quote(read_data(file)), format = "fst_tbl", deployment = "main"),
    tar_target_raw("model", quote(run_model(data)), format = "qs")
  )
}
```

???

The mechanism behind `stantargets` is a domain-specific design pattern the I am calling the Target Factory. A target factory is a function that produces one or more target objects, where each target object is a step in the pipeline.

---

## Target factories simplify pipeline construction.

```{r, eval = FALSE}
# _targets.R
library(targets)
library(yourExamplePackage)
list(
  target_factory("data.csv")
)
```

```{r, eval = FALSE}
# R console
tar_manifest(fields = command)
#> # A tibble: 3 x 2
#>   name  command          
#>   <chr> <chr>            
#> 1 file  "\"data.csv\""   
#> 2 data  "read_data(file)"           
#> 3 model "run_model(data)"
```

???

If a target factory is part of a package, anyone else can just reuse it in their own pipelines, and get several targets for the price of one, and all the functions those targets depend on are already taken care of, as part of that same package.

---

## Example: {stantargets}

<center>
<image src="./images/stantargets.png" height = "300px">
</center>

* Easy pipeline construction for Stan statistical models.
* Uses R packages [`cmdstanr`](https://mc-stan.org/cmdstanr/) and [`posterior`](https://mc-stan.org/posterior/).

???

{stantargets} was the first package I wrote with target factories. Its purpose is to make Bayesian data analysis easier for statisticians who use Stan.

---

## About Stan

* Probabilistic programming language ([Carpenter et al. 2017](https://www.jstatsoft.org/article/view/v076i01)).
* Markov chain Monte Carlo (MCMC) with HMC and NUTS.
    * Often more efficient than Gibbs sampling.
    * Flexible specification of posterior distributions.
    * Indifferent to conjugacy.
* Variational inference (ADVI)
* Penalized MLE (L-BFGS)

???

Stan is a probabilistic programming language for all kinds of statistical modeling. It is most famous for Hamiltonian Monte Carlo to fit Bayesian models, but it also variational inference and optimization. It can be really fast, but as with anything Bayesian, it comes with a sizable computational burden. If you use Stan and if you use R, then {stantargets} is worth a try.

---

## Example: Bayesian longitudinal model for clinical trials

<!--
$$
\begin{aligned}
& y \sim \text{MVN}(X_{(n \cdot t) \times p} \beta, \ I_{n \times n} \otimes \Sigma_{t \times t} ) \\
& \qquad \beta \sim \text{MVN} (0, 10^2 I_{p \times p})\\
&  \qquad \Sigma_{t \times t} = \left (I_{t \times t} \sigma \right ) \Lambda_{t \times t} \Lambda_{t \times t}' \left (I_{t \times t} \sigma \right ) \\
& \qquad \qquad \sigma_1, \ldots, \sigma_t \stackrel{\text{ind}}{\sim} \text{Cauchy}^+(0, 5) \\
& \qquad \qquad \Lambda_{t \times t}\Lambda_{t \times t}' \sim \text{LKJ}(\text{shape} = 1, \text{order} = t)
\end{aligned} 
$$
-->

![](./images/model.png)

* A common variant of this model uses inverse-Wishart for the covariance, which induces troublesome prior relationships among covariance components ([Alvarez et al. 2016](https://arxiv.org/abs/1408.4050)).
* The above LKJ-based model could help refine some existing models currently used on real clinical trial data. 
* **But first, we need to ensure the above model is implemented correctly.**

???

We're going to focus on this Bayesian longitudinal linear model. It's like an MMRM but without random effects, and this general kind of model is extremely popular in clinical trial data analysis. But most of the time, you see it with an inverse-Wishart prior if the covariance is unstructured, which induces problematic associations among variances and correlations. A solution is to separately model variances and correlations and to use something like an LKJ prior on the correlation matrix. So I wrote this model in Stan and wanted to try it out on clinical data. But first, we should check that this model is implemented correctly.

---

## Interval-based validation study

* For several independent replications:
    * Simulate data from the prior predictive distribution.
    * Fit the model to the simulated data using MCMC.
    * Calculate x% posterior intervals for each scalar parameter.
* For each of scalar parameter, roughly x% of the posterior intervals should cover the corresponding parameter draws from the joint prior.
* 50% and 95% are common choices for x%.
* Based on the concept of calibration ([Carpenter 2017](https://statmodeling.stat.columbia.edu/2017/04/12/bayesian-posteriors-calibrated/)).
* Simulation-based calibration extends this idea further ([Cook et al. 2006](https://www.jstor.org/stable/27594203); [Talts et al. 2020](https://arxiv.org/abs/1804.06788)).

???

One way to do that is with calibration. You simulate data from the prior predictive distribution, analyze each dataset with the model, and find out how well the posterior parameter samples agree with the parameter samples from the prior. So if we compute a 50% posterior interval for each simulation, then 50% of those intervals should contain the corresponding prior draws of that parameter. Likewise for 95% intervals. If coverage is nominal, that's evidence that the model is implemented correctly. In practice, I have found this exercise to uncover a lot of bugs, and I will walk through it in a moment.

---

## Write the pipeline in Target Markdown

* R Markdown interface for `targets`.
* Interactive mode for prototyping and emulation.
* Non-interactive mode for pipeline construction.
* Template available through RStudio and [`use_targets()`](https://docs.ropensci.org/targets/reference/use_targets.html).

<center>
<img src="./images/target_markdown.png" height="350" align = "center">
</center>

???

When I do, I will use Target Markdown, a brand new system that combines the best of {targets} with the best of R Markdown. We want the power of `stantargets` to run a huge simulation pipeline, and we also want everything to live inside an R Markdown document because it's convenient and because we can explain the details of the methodology right next to the actual code that runs it.

There are two ways to use Target Markdown. There's an interactive mode for testing and prototyping, and there's a non-interactive mode for pipeline construction. What this looks like is you'll just use R Markdown pretty much like you would in other situations, but you have a special {targets} language engine that creates a pipeline behind the scenes. And this pipeline is created code chunk by code chunk.

If you have the latest version of {targets}, you can get an example Target Markdown document either through the RStudio template system or through the use_targets() function.

---

## One function to simulate prior predictive data

* No other user-defined function required.
* Interactive mode emulates `targets`' behavior in your local environment.

<center>
<img src="./images/target_markdown_globals.png" height = "400">
</center>

???

For our Bayesian longitudinal model of clinical trial data, we're in our R Markdown report, and we want to define a function to simulate data from the prior predictive distribution. Thanks to {stantargets}, is the only user-defined function that the pipeline needs. To make it available to the pipeline, we use the {targets} language engine instead of the R language engine, and we set the tar_globals chunk option to TRUE. If you're in the notebook interface and you click the green "play" button on the right, Target Markdown will just run the code and assign it to the environment of the pipeline like it says here at the bottom. You can do this if you want to test and prototype the function locally in your R session.

Interactive mode for a target or list of targets is similar. Target Markdown will resolve the directed acyclic graph, run the correct targets in the correct order, test that the targets can be stored and retried properly, and then assign them to memory as R variables.

---

## Simulation and MCMC with {stantargets}

* Non-interactive mode writes the `_targets.R` file and supporting scripts.
* Declares targets but does not run them.

<center>
<img src="./images/target_markdown_targets.png" height = "400">
</center>

???

But if you the whole report with the Knit button, or if you disable interactive mode manually, the chunk will run in non-interactive mode. That means instead of actually running the code now, Target Markdown will write special R scripts to build on a persistent pipeline that contains the functions or targets defined in the code chunk. This message at the bottom tells you where those scripts were written. These two modes let can seamlessly transition back and forth between prototyping locally and seriously running the pipeline.

In this chunk, we invoke a target factory called tar_stan_mcmc_rep_summary() to define the bulk of the work of this simulation study: draw prior predictive data, run the model, and compute summary statistics and convergence diagnostics. 

---

## Simple target for convergence diagnostics

<center>
<img src="./images/target_markdown_convergence.png">
</center>

???

You can even turn arbitrary code chunks into targets with the tar_simple chunk option. It will work as long as the chunk acts like a pure function, meaning it returns a value and does not cause any side effects. So here we have one target to summarize convergence diagnostics,

---

## Simple target for coverage statistics

<center>
<img src="./images/target_markdown_coverage.png">
</center>

???

and another target to calculate coverage.

At this point, our entire pipeline is defined. If we were to just stick with the code chunks above, we could click the Knit button to write the entire pipeline and then launch another session outside the report to actually run the pipeline.

---

## Optional R code chunk to run the pipeline

* Either run the pipeline in an ordinary R code chunk (below) or invoke `tar_make_clustermq()` outside the R Markdown report.

<center>
<img src="./images/target_markdown_run.png">
</center>

???

Alternatively, you could define an R code chunk later on to actually run the pipeline inside the report itself. Here, we switch back to the R language engine because we are running the pipeline instead of setting it up.

---

## Optional R code chunks to read the results

<center>
<img src="./images/target_markdown_results.png">
</center>

???

We could also have R code chunks to read and display the results of the pipeline.

---


## First run takes a long time.

<center>
<img src="./images/tar_make_clustermq.png">
</center>

???

The first time I ran this pipeline, it took almost 7 hours to finish, even though {targets} distributed the simulations across 100 workers of a Sun Grid Engine cluster. That's how big these computations can get.

---

## Subsequent runs skip up-to-date targets.

<center>
<img src="./images/tar_make_clustermq_skip.png">
</center>

???

But the second time around, all the results were up to date, and it took only a few seconds for the whole report to re-render.

---

## Convergence diagnostics

<center>
<img src="./images/tar_read_convergence.png">
</center>

???

And the last section of the rendered report shows the results. Convergence diagnostics looked good, there was only one simulation out of 1000 with any potential scale reduction factor above 1.01.

---

## Coverage is nominal.

<center>
<img src="./images/tar_read_coverage.png" height = "500">
</center>

???

And coverage looks nominal on average. Overall, 50% of the 50% posterior intervals covered the truth, and 95% of the 95% posterior intervals covered the truth, which is evidence of good calibration. So we have a complete, well-documented story wrapped up in an R Markdown document backed by a powerful {targets} pipeline. 

---

## Resources

Resource | Link
---|---
Slides | <https://wlandau.github.io/rmedicine2021-slides/>
Slide source | <https://github.com/wlandau/rmedicine2021-slides>
Pipeline report | <https://wlandau.github.io/rmedicine2021-pipeline/>
Pipeline source | <https://github.com/wlandau/rmedicine2021-pipeline>
`targets` | <https://docs.ropensci.org/targets/>
Target Markdown | <https://books.ropensci.org/targets/markdown.html>
`stantargets` |  <https://docs.ropensci.org/stantargets/>
Stan | <https://mc-stan.org/>
`cmdstanr` | <https://mc-stan.org/cmdstanr/>
`posterior` | <https://mc-stan.org/posterior/>

???

These slides are publicly available, and so is the fully rendered version of the Target Markdown report I showed today. In these links you can also find the source code of those materials and the various packages I mentioned.

---

## Thanks

* `stantargets`: Melina Vidoni served as editor and Krzysztof Sakrejda and Matt Warkentin served as reviewers during the rOpenSci software review process.
* Target Markdown: Christophe Dervieux and Yihui Xie provided crucial advice during initial development.
* Richard Payne and Karen Price reviewed this Bayesian model validation project.

???

I would like to thank everyone who helped out with {targets} and its ecosystem, especially the folks here who helped make {stantargets} and Target Markdown possible. Thanks also to the R Medicine Program Committee for allowing me to speak, and thank you to everyone listening.

---

## References

.small[
* Alvarez, Ignacio, Jarad Niemi, and Matt Simpson. 2016. "Bayesian Inference for a Covariance Matrix." http://arxiv.org/abs/1408.4050.
* Bürkner P, Gabry J, Kay M, Vehtari A (2021). "posterior: Tools
for Working with Posterior Distributions." R package version
0.1.6, <https://mc-stan.org/posterior>.
* Carpenter, Bob. 2017. "Bayesian Posteriors are Calibrated by Definition". Statistical Modeling, Causal Inference, and Social Science. <https://statmodeling.stat.columbia.edu/2017/04/12/bayesian-posteriors-calibrated/>
* Carpenter, Bob, Andrew Gelman, Matthew D. Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. 2017. Stan: A probabilistic programming language. Journal of Statistical Software 76(1). [10.18637/jss.v076.i01](https://www.jstatsoft.org/article/view/v076i01).
* Cook, Samantha R., Andrew Gelman, and Donald B. Rubin. 2006. "Validation of Software for Bayesian Models Using Posterior Quantiles." Journal of Computational and Graphical Statistics 15 (3): 675–92. http://www.jstor.org/stable/27594203.
* Gabry, Jonah, and Rok Češnovar (2021). cmdstanr: R Interface
  to 'CmdStan'. https://mc-stan.org/cmdstanr,
  https://discourse.mc-stan.org.
* Gelman, Andrew. 2006. "Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper)." Bayesian Analysis 1 (3): 515–34. https://doi.org/10.1214/06-BA117A.
* Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2014. Bayesian Data Analysis. Edited by Francesca Dominici, Julian J. Faraway, Martin Tanner, and Jim idek. Third. CRC Press.
* Landau, William Michael. 2021a. "The Stantargets R Package: A Workflow Framework for Efficient Reproducible Stan-Powered Bayesian Data Analysis Pipelines." Journal of Open Source Software 6 (60): 3193. https://doi.org/10.21105/joss.03193.
* ———. 2021b. "The Targets R Package: A Dynamic Make-Like Function-Oriented Pipeline Toolkit for Reproducibility and High-Performance Computing." Journal of Open Source Software 6 (57): 2959. https://doi.org/10.21105/joss.02959.
* Schubert, Michael. 2019. "clustermq enables efficient parallelization of genomic analyses." Bioinformatics 35 (21): 4493–95. https://doi.org/10.1093/bioinformatics/btz284.
* Talts, Sean, Michael Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman. 2020. "Validating Bayesian Inference Algorithms with Simulation-Based Calibration." http://arxiv.org/abs/1804.06788.
* Xie, Yihui, J.J. Allaire, and Garrett Grolemund (2018). R Markdown: The Definitive Guide. Chapman and Hall/CRC. ISBN 9781138359338. https://bookdown.org/yihui/rmarkdown.
* Xie, Yihui, Christophe Dervieux, and Emily Riederer (2020). R Markdown Cookbook. Chapman and Hall/CRC. ISBN 9780367563837. https://bookdown.org/yihui/rmarkdown-cookbook.
]
